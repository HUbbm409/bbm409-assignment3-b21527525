{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning 3rd Assignment Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II\n",
    "\n",
    "### What\n",
    "We are building a neural network and neural network interface to classify images of flowers. For that reason we are creating single and multi layer networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How\n",
    "Let's talk about what steps I followed for this problem. \n",
    "#### Structure"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Neural Network\n",
    "├── data\n",
    "│   ├── code\n",
    "│   │   │   \n",
    "│   │   ├── readImage.py\n",
    "│   │   ├── test.mat\n",
    "│   │   ├── train.mat\n",
    "│   │   └── validation.mat\n",
    "│   │\n",
    "│   └── flowers\n",
    "│       │\n",
    "│       .\n",
    "│       .\n",
    "│       .\n",
    "│\n",
    "├── nn\n",
    "│   ├── __init__.py\n",
    "│   ├── layer.py\n",
    "│   ├── math_.py\n",
    "│   ├── neuralnetwork.py\n",
    "│   └── node.py\n",
    "│\n",
    "├── report\n",
    "│   └── report.ipynb\n",
    "│\n",
    "├── test.py\n",
    "├── model.json\n",
    "└── train.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Firstly I had to create project structure. I created a python package for neural network interface and one for data. The data package was ready anyway so I just added methods to load the files. For neural network I wanted to create a dynamic network so I implemented a json loader method. From json template one can create different kinds of network similar to the ones in caffe.\n",
    "#### Preparing data\n",
    "Data is vectorized greyscale images of certain flower types. That handled most of the job. I took image vectors then squashed their intensity values between 0 and 1. Then I hot encoded their labels so I can classify them.\n",
    "I did same thing for validation data so that I can validate as the model trained.\n",
    "#### Regularization\n",
    "I have tried few regularization methods like __l1__, __l2__ and __dropout__ regularization. None of them worked out well. Perhaps I failed to implement them or maybe model wasn't suitable for this kind of task. Anyway, I only left dropout since I implemented others pretty early on in a desperate attempt to improve the validation scores. When I saw that they actually worsened the situtation(underflows) I removed them and never looked back. At this point I believe it is just waste of time to reimplement them since they do nothing positive on the model. <br>\n",
    "Let's talk about dropout specifically. It is the only regularization required a seperate method, meaning I could turn it on and off at will without much work. I added dropout method to the layers and it is called during forward pass after activation functions called. Neural network has a parameter called dropout_probabilty which used during dropout sequence. I used 0.5 mostly and never really tried anything else. During training with dropout, obviously, training scores drops significantly since half of the neurons not working during prediction while validation accuracy does not change a lot. \n",
    "#### Losses and Activations\n",
    "I put every loss and activation function on seperate file called 'math_'. \n",
    "List of currently available activation functions:\n",
    " - __Sigmoid__: You can't go wrong with sigmoid. I polished it a lot so it is best implemented one out of the rest.\n",
    " - __Tanh__: Gives similar result to sigmoid. \n",
    " - __Softmax__: Only used on output layer. Works pretty fine.\n",
    " - __ReLu__: Problem child. Because of dying relu problem you will quickly get all 0 outputs.\n",
    " - __Leaky ReLu__: In an attempt to solve the dying relu problem I added this. Never got it to work because of the overflows. Can't really care at this point.\n",
    " \n",
    "I also have method for signum but I did not test it so you can't use it. <br>\n",
    "List of currently available loss functions:\n",
    "\n",
    " - __Cross entropy__: First added loss function. Works just fine.\n",
    " - __Sum of squares__: Works ok, but did not tested it extensively.\n",
    " - __Mean squared error__: Implementation is derived from sum of squares and derivative calculated by me. So possibly does not work correctly.\n",
    " - __R Squared Error__: I added this last. While I was working with scikit learn I saw this and tried to implement it myself. Currently gives only negative output somehow so I suggest not using it.\n",
    " \n",
    "Moral of the story, stick with the cross entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "#### Analysis of activation functions\n",
    "\n",
    "#### Analysis of Loss functions\n",
    "\n",
    "#### Analysis of dropout regularization\n",
    "\n",
    "#### Analysis of training/validation scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis of activation functions\n",
    "\n",
    " - __Sigmoid:__\n",
    " \n",
    "  > Sigmoid is, first activation I have ever implemented. I tried few algorithms on it and currently using the one I found internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
